---ntitle: 应用nlayout: postnshare: falsen---

本章，我们将介绍如何使用深度学习解决计算机视觉、语音识别、自然语言处理以及其它商业领域中的应用。
首先我们讨论在许多最重要的AI应用中所需的大规模神经网络的实现。
接着，我们回顾深度学习已经成功应用的几个特定领域。
尽管深度学习的一个目标是设计能够处理各种任务的算法，然而截止目前应用深度学习仍然需要一定程度的特殊化。
例如，计算机视觉中的任务对每一个样本都需要处理大量的输入特征（像素）。
自然语言处理任务的每一个输入特征都需要对大量的可能值（词汇表中的词）建模。
<!-- % 431 -->

n# 大规模深度学习n
<!-- % 431  -->

深度学习的基本思想基于联结主义：尽管机器学习模型中单个生物性的神经元或者说是单个特征不是智能的，但是大量的神经元或者特征作用在一起往往能够表现出智能。
我们必须着重强调神经元数量必须\emph{很大}这个事实。
相比20世纪80年代，如今神经网络的精度以及处理任务的复杂度都有一定提升，网络规模的巨大提升是一个关键的因素。
正如我们在\sec?中看到的一样，在过去的三四十年中，网络规模是以指数级的速度递增的，尽管如今的人工神经网络的规模也仅仅和昆虫的神经系统差不多。

大规模神经网络的必要性，所以深度学习需要高性能的硬件设施和软件实现。
<!-- % 431 -->

n## 快速的CPU实现n

传统的神经网络是用单台机器的CPU来训练的。
如今，这种做法通常被视为是不可取的。
现在，我们通常使用GPU或者许多台机器的CPU连接在一起进行计算。
在使用这种昂贵配置之前，为论证CPU无法承担神经网络所需的巨大计算量，研究者们付出了巨大的努力。
<!-- % 432 head -->


描述如何实现高效的数值CPU代码已经超出了本书的讨论范围，但是我们在这里还是要强调通过设计一些特定的CPU上的操作可以大大提升效率。
例如，在2011年，最好的CPU在训练神经网络时使用定点运算能够比浮点运算跑的更快。
通过调整定点运算的实现方式，{Vanhoucke-et-al-2011}获得了相对一个很强的浮点运算系统3倍的加速。
因为各个新型CPU都有各自不同的特性，所以有时候采用浮点运算实现会更快。
一条重要的准则就是通过特殊设计的数值运算可以获得巨大的回报。
除了选择定点运算或者浮点运算以外，还包括其它的策略，如通过优化数据结构避免高速缓存缺失、使用向量指令等。
如果模型规模不会限制模型表现（不会影响模型精度）时，机器学习的研究者们一般忽略这些实现的细节。
<!-- % 432 -->

n## GPU 实现n

许多现代神经网络的实现基于图形处理器。
GPU是一种特殊设计的硬件，设计的原始目的是为了处理图形应用。
视频游戏系统的消费市场刺激了图形处理硬件的发展。
它为视频游戏所设计的特性也可以使神经网络的计算受益。
<!-- % 432   -->

视频游戏要求许多操作能够快速并行地实现。
环境和角色模型通过一系列顶点的3D坐标确定。
为了将大量的3D坐标转化为2D显示器上的坐标，显卡必须快速实现矩阵乘法或者除法。
之后，显卡必须并行地实现每个像素上的计算来确定每个像素点的颜色。
在这两种情况下，计算都是非常简单的，并且不涉及CPU通常遇到的复杂的分支运算。
例如，同一个刚体内的每个顶点都会乘上相同的矩阵；也就是说，不需要通过{\tt if}判断确定顶点需要乘哪个矩阵。
计算过程也是完全相互独立的，因此也能够并行操作。
计算过程还涉及处理大量内存缓冲以及描述每一个对象的纹理（颜色模式）如何渲染的位图信息。
总的来说，这使显卡设计为拥有高度并行特性以及很高的内存带宽，同时也付出了一些代价，如相比传统的CPU更慢的时钟速度以及更弱的处理分支运算的能力。
<!-- % p 433 -->


与上述的实时图形算法相比，神经网络算法需要的是相同的性能特性。
神经网络算法通常涉及大量参数、激活值、梯度值的缓冲区，其中每个值在每一次训练迭代中都要被完全更新。
这些缓冲太大，会超出传统的桌面计算机的高速缓存(cache)，所以内存带宽通常会成为主要瓶颈。
相比CPU，GPU极高的内存带宽成为了一个显著的优势。
神经网络的训练算法通常并不涉及分支运算和复杂的控制指令，所以更适合在GPU硬件上训练。
由于神经网络能够被分为多个单独的"神经元"，并且独立于同一层内其他神经元进行处理，所以神经网络可以从GPU的并行特性中受益匪浅。
<!-- % 433 -->


GPU硬件最初专为图形任务而设计。
随着时间的推移，GPU也变得更灵活，允许定制的子程序处理转化顶点坐标或者计算像素颜色的任务。
原则上，GPU不要求这些像素值实际基于渲染任务。
将计算的输出值作为像素值写入缓冲区，GPU可以用于科学计算。
{Steinkrau2005}在GPU上实现了一个两层全连接的神经网络，并获得了相对基于CPU的基准方法三倍的加速。
不久以后，{chellapilla:inria-00112631}也论证了相同的技术可以用来加速训练监督卷积网络。
<!-- % 433 -->


使用显卡训练神经网络的热度在通用GPU发布以后开始爆炸性增长。
这种通用GPU可以执行任意的代码，而并非仅仅渲染子程序。
NVIDIA的CUDA编程语言使得我们可以用一种像C一样的语言实现任意代码。
由于相对简便的编程语言，强大的并行能力以及巨大的内存带宽，通用GPU为我们提供了训练神经网络的理想平台。
在它发布以后不久，这个平台就迅速被深度学习的研究者们接纳了{cite?}。



如何在通用GPU上写高效的代码依然是一个难题。
在GPU上获得很好表现所需的技术与CPU上的技术非常不同。
比如说，基于CPU的良好代码通常被设计为尽可能从高速缓存中读取更多的信息。
然而在GPU中，大多数可写内存位置并不会被高速缓存，所以计算某个值两次往往会比计算一次然后从内存中读取更快。
GPU代码是天生多线程的，不同线程之间必须仔细协调好。
例如，如果能够把数据级联起来，那么涉及内存的操作一般会更快。
当几个线程同时需要读/写一个值的时候，像这样的级联会作为一次内存操作出现。
不同的GPU可能采用不同的级联读/写数据的方式。
通常来说，如果在$n$个线程中，线程$i$对应的是第$i+j$处的内存，其中$j$是$2$的某个幂的倍数。
具体的设定在不同的GPU型号中有所区别。
GPU另一个常见的设定是使一个组中的所有线程都同时执行同一指令。
这意味着GPU上的分支操作是很困难的。
线程被分为一个个称作warp的小组。
在一个warp中的每一个线程在每一个循环中执行同一指令，所以当同一个warp中的不同线程需要执行不同的指令时，需要使用串行而非并行的方式。
<!-- % 434  -->


由于实现高效GPU代码的困难性，研究人员应该构建他们的工作流程，避免对每一个新的模型或算法都编写新的GPU代码。
通常来讲，人们会选择建立一个包含高效操作（如卷积和矩阵乘法）的软件库解决这个问题，然后再从库中调用所需要的操作确定模型。
例如，机器学习库Pylearn2 {cite?}通过调用Theano {cite?}和cuda-convnet {cite?}提供的高性能操作，囊括了许多机器学习算法。
这种分解方法还可以简化对多种硬件的支持。
例如，同一个Theano程序可以在CPU或者GPU上运行，而不需要改变调用Theano的方式。
其它库如Tensorflow{cite?}和Torch{cite?}也提供了类似的功能。
<!-- % 434 end -->


n## 大规模的分布式实现n

在许多情况下，单个机器的计算资源是有限的。
因此我们希望把训练或者推断的任务分摊到多个机器上进行。
<!-- % 434 end -->

分布式的推断是容易实现的，因为每一个输入的样本都可以在单独的机器上运行。
这也被称为数据并行。

同样的，模型并行也是可行的，其中多个机器共同运行一个数据点，每一个机器负责模型的一个部分。
对于推断和学习，这都是可行的。
<!-- % 435 -->


训练中，数据并行在某种程度上说更难。
对于随机梯度下降的单步来说，我们可以增加minibatch的大小，但是从优化性能的角度来说，通常我们得到反馈少于线性的反馈。
使用多个机器并行地计算多个梯度下降步是一个更好的选择。
不幸的是，梯度下降的标准定义完全是一个串行的过程：
第$t$步的梯度是第$t-1$步所得参数的函数。
<!-- % 435 -->


这个问题可以使用异步随机梯度下降{cite?}解决。
在这个方法中，几个处理器的核共用存有参数的内存。
每一个核在无锁情况下读取了这些参数，然后计算对应的梯度，然后在无锁状态下更新了这些参数。
这种方法减少了每一个梯度下降所获得的平均提升，因为一些核把其它的核所更新的参数（写）覆盖了。
但因为更新步数的速率增加，总体上还是加快了学习过程。
{Dean-et-al-NIPS2012}率先提出了多机器无锁的梯度下降方法，其中参数是由参数服务器管理而非存储在共用的内存中。
分布式的异步梯度下降方法依然是训练深度神经网络的基本方法，被工业界很多机器学习组所使用{cite?}。
学术界的深度学习研究者们通常无法负担那么大规模的分布式学习系统，但是一些研究关注于如何在校园环境中相对较廉价的硬件系统中构造分布式网络{cite?}。
<!-- % 435 -->


n## 模型压缩n
<!-- % 435  -->

在许多商业应用的机器学习模型中，一个时间和内存开销较小的推断算法比一个时间和内存开销较小的训练算法要更为重要。
对于那些不需要个性化设计的应用来说，我们只需要一次性的训练模型，然后它就可以被成千上万的用户使用。
在许多情况下，相比开发者，终端用户的可用资源往往更有限。
例如，研究者们可以使用巨大的计算机集群训练一个语音识别的网络，然后发布到移动手机上。


减少推断所需开销的一个关键策略是模型压缩{cite?}。
模型压缩的基本思想是用一个更小的模型取代替原始耗时的模型，只需更少内存和运行时间来存储和评估。



原始模型的规模很大，且主要为了防止过拟合时，模型压缩可以起到作用。
在许多情况下，拥有最小泛化误差的模型往往是多个独立训练而成的模型的集成。
评估所有$n$个集成成员是昂贵的。
有时候，当单个模型很大（例如，如果它使用Dropout正则化）时，其泛化能力也会很好。
<!-- % 436  -->

这些巨大的模型学习某个函数$f(\Vx)$时，但选用的参数数量超过了任务所需的参数数量。
仅仅训练样本数是有限的，所以网络的规模是受限的。
只要我们拟合了这个函数$f(\Vx)$，我们就可以生成一个拥有了无穷多训练样本的训练集，只需将$f$作用于任意生成的$\Vx$。
然后，我们使用这些样本训练一个新的更小的模型，在这些点上拟合$f(\Vx)$。
为了更加充分地利用了这个新的小模型的容量，最好能够从一个类似于真实的测试数据（后面会用到）的分布中采样$\Vx$。
这个过程可以通过损坏训练样本或者从原始训练数据训练的生成模型中采样完成。

此外，我们还可以仅在原始训练数据上训练一个更小的模型，但只是为了复制模型的其它特征，比如在不正确的类上的后验分布{cite?}。
<!-- % 436  -->

n## 动态结构n

一般来说，加速数据处理系统的一种策略是构造一个系统，这个系统用动态结构描述图中处理输入的所需计算过程。
在给定一个输入的情况中，数据处理系统可以动态地决定运行神经网络系统的哪一部分。
单个神经网络内部同样也存在动态结构，给定输入信息，决定特征（隐藏单元）哪一部分用于计算。
这种神经网络中的动态结构有时被称为条件计算{cite?}。
由于模型结构许多部分可能只跟输入的一小部分有关，只计算那些需要的特征可以起到加速的目的。
<!-- % 436  -->

动态结构计算是一种基础的计算机科学方法，广泛应用于软件工程项目。
应用于神经网络的最简单的动态结构基于决定神经网络（或者其它机器学习模型）中的哪些子集需要应用于特定的输入。
<!-- % 437  -->

在分类器中加速推断的可行策略是使用级联的分类器。
当目标是检测罕见对象（或事件）的是否存在，可以应用级联策略。
要确定对象是否存在，我们必须使用具有高容量、运行昂贵的复杂分类器。 
然而，因为对象是罕见的，我们通常可以使用更少的计算拒绝不包含对象的输入。
在这些情况下，我们可以训练一序列分类器。
序列中的第一分类器具有低容量，训练为具有高召回率。
换句话说，他们被训练为确保对象存在时，我们不会错误地拒绝输入。
最终的分类器训练为具有高精度。
在测试时，我们按照顺序运行分类器进行推断，一旦级联中的任何一个拒绝它，就选择抛弃。
总的来说，这允许我们使用高容量模型以较高的置信度验证对象的存在，而不是强制我们为每个样本付出完全推断的成本。
有两种不同的方式可以使得级联实现高容量。
一种方法是使级联中靠后的成员单独具有高容量。
在这种情况下，系统作为一个整体显然具有高容量，因为它的一些个体成员是高容量的。 
还可以使用另一种级联，其中每个单独的模型具有低容量，但是由于许多小型模型的组合，整个系统具有高容量。
{Viola01}使用级联的增强决策树实现适合在手持数字相机中使用的快速并且鲁棒的面部检测器。
本质上，它们的分类器使用滑动窗口方法来定位面部，许多窗口会被检查，如果它们不包含面部则被拒绝。
级联的另一个版本使用早期模型来实现一种硬性的注意机制：级联的早期成员定位对象，并且级联的后续成员在给定对象位置的情况下执行进一步处理。
例如，Google使用两步级联从街景视图图像中转换地址编号，首先使用一个机器学习模型查找地址编号，然后使用另一个机器学习模型将其转录{cite?}。
<!-- % 437  -->

决策树本身是动态结构的一个例子，因为树中的每个节点决定应该使用哪个子树来评估输入。
一个结合深度学习和动态结构的简单方法是训练一个决策树，其中每个节点使用神经网络做出决策{cite?}，虽然这种方法没有实现加速推断计算的目标。
<!-- % 437 end -->



类似的，我们可以使用称为选通器的神经网络来选择在给定当前输入的情况下将使用几个专家网络中的哪一个来计算输出。
这个想法的第一个版本被称为专家混合体{cite?}，其中选通器为每个专家输出一个概率或权重（通过非线性的softmax函数获得），并且最终输出由各个专家输出的加权组合获得。
