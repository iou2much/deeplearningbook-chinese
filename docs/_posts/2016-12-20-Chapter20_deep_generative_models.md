---ntitle: 深度生成模型nlayout: postnshare: falsen---
在本章中，我们介绍几种具体的生成模型，这些模型可以使用\chap?至\chap?中出现的技术构建和训练。
所有这些模型在某种程度上都代表了多个变量的概率分布。
有些模型允许显式地计算概率分布函数。
其它模型则不允许直接评估概率分布函数，但支持隐式获取分布知识的操作，如从分布中采样。
这些模型中的一部分使用\chap?中的图模型语言，从图和因子的角度描述为结构化概率模型。
其它的不能简单地从因子角度描述，但仍然代表概率分布。


n# 玻尔兹曼机n
玻尔兹曼机最初作为一种广义的"联结主义"引入，用来学习二值向量上的任意概率分布~{cite?}。
玻尔兹曼机的变体（包含其他类型的变量）早已超过了原始玻尔兹曼机的流行程度。
在本节中，我们简要介绍二值玻尔兹曼机并讨论训练模型和执行推断时出现的问题。

我们在$d$维二值随机向量$\Vx \in \{0, 1\}^d$上定义玻尔兹曼机。
玻尔兹曼机是一种基于能量的模型（\sec?），意味着我们可以使用能量函数定义联合概率分布：
 P(\Vx) = \frac{\exp(-E(\Vx))}{Z},
\end{align}
其中$E(\Vx)$是能量函数，$Z$是确保$\sum_{\Vx} P(\Vx)=1$的配分函数。
玻尔兹曼机的能量函数如下
给出：
 E(\Vx) = -\Vx^\top \MU \Vx - \Vb^\top \Vx,
\end{align}
其中$\MU$是模型参数的"权重"矩阵，$\Vb$是偏置向量。

<!-- % -- 645 -- -->

在一般设定下，给定一组训练样本，每个样本都是$n$维的。
\eqn?描述了观察到的变量的联合概率分布。
虽然这种情况显然可行，但它限制了观察到的变量和权重矩阵描述的变量之间相互作用的类型。
具体来说，这意味着一个单元的概率由其他单元值的线性模型（逻辑回归）给出。

当不是所有变量都能被观察到时，玻尔兹曼机变得更强大。
在这种情况下，隐变量类似于多层感知机中的隐藏单元，并模拟可见单元之间的高阶交互。
正如添加隐藏单元将逻辑回归转换为MLP，导致MLP成为函数的通用逼近器，具有隐藏单元的玻尔兹曼机不再局限于建模变量之间的线性关系。
相反，玻尔兹曼机变成了离散变量上概率分布律函数的通用逼近器 {cite?}。


形式地，我们将单元$\Vx$分解为两个子集：可见单元$\Vv$和隐含（或隐藏）单元$\Vh$。
能量函数变为
\begin{align}
 E(\Vv, \Vh) = -\Vv^\top \MR \Vv - \Vv^\top \MW \Vh - \Vh^\top \MS \Vh - \Vb^\top \Vv - \Vc^\top \Vh.
\end{align}
\paragraph{玻尔兹曼机的学习}玻尔兹曼机的学习算法通常基于最大似然。
所有玻尔兹曼机都具有难以处理的配分函数，因此最大似然梯度必须使用\chap?中的技术来近似。

<!-- % -- 646 -- -->

玻尔兹曼机有一个有趣的性质，当基于最大似然的学习规则训练时，连接两个单元的特定权重的更新仅取决于这两个单元在不同分布下收集的统计信息：$P_{\text{model}}(\Vv)$和$\hat{P}_{\text{data}}(\Vv) P_{\text{model}}(\Vh  \mid  \Vv)$。
