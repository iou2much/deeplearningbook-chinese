{'user': 'acct:vsooda@hypothes.is', 'text': '中文语法有误。考虑:\n显示地对标签噪声进行建模是防止这一点的一种方法。', 'origin_text': '\u2061p(y∣x)\\log p(y \\mid \\Vx)会是有害的。  !!!为了防止这一点的一种方法是显式地对标签上的噪声进行建模。!!!  例如，我们可以假设，对于一些小常数ϵϵ\\epsilon，训练集', 'time': '2017-02-03T03:29'}
{'user': 'acct:vsooda@hypothes.is', 'text': '任何其他可能的标签也可能是正确的。', 'origin_text': '训练集标记yyy是正确的概率是1−ϵ1−ϵ1-\\epsilon，  !!!任何其他可能的标签可能是正确的。!!!  这个假设很容易就能解析地与代价函数结合，而不用显式地采噪声样本', 'time': '2017-02-03T03:39'}
{'user': 'acct:fishlu@hypothes.is', 'text': '在机器学习中以增大训练误差为代价来减少测试误差的策略，统称为正则化。', 'origin_text': '表现好，并且能在新输入上泛化好的算法是机器学习中的一个核心问题。  !!!在机器学习中许多策略通过明确设计，以增大训练误差为代价来减少测试误差。这些策略统称为正则化!!!  。正如我们将看到的，深度学习工作者可以使用许多形式的正则化。', 'time': '2017-02-05T12:58'}
{'user': 'acct:fishlu@hypothes.is', 'text': '设计一个不仅在训练数据上表现好', 'origin_text': '                              !!!设计不仅在训练数据上表现好!!!  ，并且能在新输入上泛化好的算法是机器学习中的一个核心问题。在机', 'time': '2017-02-05T12:58'}
{'user': 'acct:fishlu@hypothes.is', 'text': '我们可以看到深度学习工作者已经在使用各种形式的正则化。然而，开发更有效的正则化策略还是这个领域的主要研究方向之一。', 'origin_text': '，以增大训练误差为代价来减少测试误差。这些策略统称为正则化。  !!!正如我们将看到的，深度学习工作者可以使用许多形式的正则化。事实上，开发更有效的正则化策略已成为本领域的主要研究工作之一!!!  。\\chap?介绍了泛化、欠拟合、过拟合、偏差、方差和正则化', 'time': '2017-02-05T13:05'}
{'user': 'acct:FlyingFire@hypothes.is', 'text': '机器学习的一个核心问题在于，如何使算法不仅在训练数据上表现良好，并且在新输入的样本上也有同样的性能。', 'origin_text': '                              !!!设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法是机器学习中的一个核心问题!!!  。在机器学习中许多策略通过明确设计，以增大训练误差为代价来减少', 'time': '2017-03-08T00:41'}
{'user': 'acct:FlyingFire@hypothes.is', 'text': '许多机器学习中精心设计的策略目标都在于减少测试误差(test error)，与之相伴的就是有可能带来训练误差的增加。', 'origin_text': '现好，并且能在新输入上泛化好的算法是机器学习中的一个核心问题。  !!!在机器学习中许多策略通过明确设计，以增大训练误差为代价来减少测试误差!!!  。这些策略统称为正则化。正如我们将看到的，深度学习工作者可以', 'time': '2017-03-08T00:44'}
{'user': 'acct:FlyingFire@hypothes.is', 'text': '参数、范数、惩罚', 'origin_text': '。现在我们回顾几种创建这些大型深度正则化模型的策略。  !!!参数范数惩罚!!!  正则化在深度学习的出现前就已经应用了数十年。线性模型，如线', 'time': '2017-03-08T00:57'}
{'user': 'acct:FlyingFire@hypothes.is', 'text': '许多正则化方法都是基于限制模型的能力，如在神经网络、线性回归或者逻辑回归中，通过对目标函数J增加一个参数范数惩罚项Ω(θ)Ω(θ)|Omega(\\Vtheta)。', 'origin_text': '型，如线性回归和逻辑回归可以使用简单、直接有效的正则化策略。  !!!许多正则化方法通过对目标函数 JJJ添加一个参数范数惩罚Ω(θ)Ω(θ)\\Omega(\\Vtheta)，限制模型（如神经网络、线性回归或逻辑回归）的学习能力!!!  。我们将正则化后的目标函数记为J~J~\\tilde{J}：J', 'time': '2017-03-08T01:01'}
{'user': 'acct:FlyingFire@hypothes.is', 'text': '当梯度为0时，J^J^\\hat J取得最小', 'origin_text': '们可以得出的一个最小点，我们可以得出\\MH$是半正定的结论。  !!!当J^J^\\hat J取最小时，其梯度!!!  ∇wJ^(w)=H(w−w∗)∇wJ^(w)=H(w−w∗)\\', 'time': '2017-03-08T01:22'}
{'user': 'acct:FlyingFire@hypothes.is', 'text': '规模', 'origin_text': '减是权重衰减最常见的形式，我们还可以使用其他的方法惩罚模型参数的  !!!大小!!!  。另一种选择是使用L1L1L^1正则化。对模型参数ww\\V', 'time': '2017-03-08T01:27'}
{'user': 'acct:FlyingFire@hypothes.is', 'text': '而其他在梯度为0的情况下会使用解析解', 'origin_text': '性回归实例。许多不同的优化过程是可能的，有些可能会利用梯度下降  !!!而其他可能使用梯度为0的解析解!!!  ，但在所有程序中αα\\alpha在Ω(θ)>kΩ(θ)>k\\Om', 'time': '2017-03-08T01:59'}
{'user': 'acct:friskit@hypothes.is', 'text': '应该是\\mu\n\n貌似是笔误', 'origin_text': ' \\Vx, \\Vmu)。关于所有掩码的算术平均值由下式给出∑  !!!u!!!  p(μ)p(y∣x,μ),∑up(μ)p(y∣x,μ),\\beg', 'time': '2017-03-13T04:42'}

=============================   Replies   =============================

